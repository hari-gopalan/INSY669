{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Note Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What the Scraper is Doing\n",
    "\n",
    "The script performs the following tasks:\n",
    "\n",
    "- **Input and Output CSVs**: It reads URLs from an input CSV file and writes the extracted data to an output CSV file.\n",
    "- **Fetching Web Page Content**: For each URL, it sends an HTTP GET request, simulating a browser to avoid being blocked.\n",
    "- **Parsing HTML**: It uses BeautifulSoup to parse the HTML content and searches for specific tags (`span` with class `y-css-kw85nd`).\n",
    "- **Extracting Data**: It retrieves text within anchor (`<a>`) tags nested inside the targeted `span` elements, limited to 5 words per URL.\n",
    "- **Handling Errors**: The script includes error handling for failed requests, ensuring that the script doesn't crash if a URL is unreachable.\n",
    "- **Writing to CSV**: The extracted words are written to specific columns (`Label 1` to `Label 5`) in the output CSV.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. How to Inspect a Website and Find the Information to Extract\n",
    "\n",
    "When you want to scrape data from a website, you first need to understand how the information is structured in the HTML. Here's a step-by-step guide on how to do this:\n",
    "\n",
    "##### 1. Visual Inspection:\n",
    "\n",
    "- **Start by Visiting the Webpage**: Open the webpage in a browser (e.g., Chrome, Firefox).\n",
    "- **Identify the Data You Need**: Look at the page and decide what specific information you want to extract (e.g., product names, prices, reviews, etc.).\n",
    "\n",
    "##### 2. Inspecting the HTML Code:\n",
    "\n",
    "- **Right-Click on the Desired Data**: Find the piece of data you want on the page (e.g., the name of a restaurant). Right-click on it and select \"Inspect\" or \"Inspect Element\" from the context menu.\n",
    "\n",
    "Note: Make sure that the HTML code that you are viewing is for the web browser and not the mobile version. You can change the version by clicking on the dimension (right above the screen once the HTML code is opened) and selecting the right version. \n",
    "\n",
    "\n",
    "- **Examine the HTML Panel**: This opens the developer tools in your browser, highlighting the HTML element that corresponds to the data you selected.\n",
    "  - **Understanding Tags and Attributes**: HTML is made up of various tags like `<div>`, `<span>`, `<a>`, etc. Each tag can have attributes like `class`, `id`, `href`, etc., that help identify and style it.\n",
    "\n",
    "##### 3. Finding Patterns:\n",
    "\n",
    "- **Look for Classes or IDs**: Check if the element has a unique class or id. These attributes are usually consistent across similar elements on the page (e.g., all product names might be in `<span>` tags with a class `product-title`).\n",
    "- **Navigate Up or Down the HTML Structure**: Sometimes, the data is nested inside another tag. You might need to go one level up or down in the HTML hierarchy to find a container that groups several elements you’re interested in.\n",
    "- **Identify Repeating Patterns**: If you’re scraping multiple items (like a list of products), find the common pattern in the HTML structure that repeats for each item.\n",
    "\n",
    "##### 4. Testing Your Selection:\n",
    "\n",
    "- **Use the Console for Testing**: In the browser's developer tools, you can use the Console to write small JavaScript snippets to see if your selection correctly identifies all desired elements.\n",
    "- **Copy the Selector**: Once you’ve identified the correct tag and class or id, you can write the selector in your code to target these elements.\n",
    "\n",
    "This process helps you translate what you see on the webpage into code that can reliably extract the information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incomplete Scraper Script for Students\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "# Function to extract specific words from a given URL\n",
    "def extract_words_from_url(url):\n",
    "    try:\n",
    "        # Create a session object to persist settings across requests\n",
    "        session = requests.Session()\n",
    "\n",
    "        #Retry logic to handle server errors\n",
    "        retries = Retry(\n",
    "            total = 5, \n",
    "            backoff_factor = 1,\n",
    "            status_forcelist = [500, 502, 503, 504] #accomodates for various error codes\n",
    "        )\n",
    "\n",
    "        #Retry adapter classes\n",
    "        session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "        #Mimic Browser Headers  \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "        }\n",
    "\n",
    "        #Request to the URL using session.get\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Initialize an empty list to store extracted words\n",
    "        words = []\n",
    "\n",
    "        #Use .find_all to locate all <span> and <a> tags and extract keywords. \n",
    "\n",
    "        span_search = soup.find_all('span', class_='class_name')\n",
    "        for span in span_search: \n",
    "            link = span.find_all('a')\n",
    "            for link in links: \n",
    "                words.append(link.get_text().strip()) #Find all span elements with a specific class\n",
    "\n",
    "        # Return the first 5 words or fewer if less\n",
    "        return words[:5]\n",
    "\n",
    "    except RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")  # Log the error message\n",
    "        return [\"\"] * 5  # Return a list of 5 empty strings if an error occurs\n",
    "\n",
    "    finally:\n",
    "        time.sleep(1) #added a delay based on code seen to avoid overwhelming the server\n",
    "\n",
    "def scrape_urls_from_csv(input_csv, output_csv):\n",
    "    # Open the input CSV for reading and the output CSV for writing\n",
    "    with open(input_csv, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "        # TODO: Use csv.DictReader to read the CSV file and get a list of dictionaries, \n",
    "        # where each row is represented as a dictionary with column headers as keys.\n",
    "        \n",
    "        \n",
    "        # Get the fieldnames (i.e., the column headers from the input CSV)\n",
    "        fieldnames = reader.fieldnames\n",
    "\n",
    "        # TODO: Use csv.DictWriter to write rows into the output CSV file, \n",
    "        # making sure to include the same fieldnames as the input CSV.\n",
    "        \n",
    "\n",
    "        # Write the headers (column names) into the output CSV file\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Iterate over each row in the input CSV\n",
    "        for row in reader:\n",
    "            # TODO: Extract the URL from the 'restaurant_url' column in the CSV.\n",
    "            \n",
    "            # If the URL is missing, skip the row and print a message\n",
    "            if not url:\n",
    "                print(f\"Skipping row with missing URL: {row}\")\n",
    "                continue\n",
    "\n",
    "            # Print the URL being processed for debugging purposes\n",
    "            print(f\"Processing URL: {url}\")\n",
    "\n",
    "            # TODO: Call the extract_words_from_url function to get a list of words\n",
    "            \n",
    "\n",
    "            # TODO: For each word in the extracted list, assign it to a new column (e.g., 'Label 1', 'Label 2', etc.)\n",
    "            # Ensure the columns are named 'Label 1', 'Label 2', etc., and each word is assigned to the correct column.\n",
    "            \n",
    "\n",
    "            # Write the updated row into the output CSV\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # TODO: Add a small delay between processing each URL to avoid overwhelming the server\n",
    "            # Hint: A delay of 1 second could be more appropriate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: Specify the input and output CSV file paths\n",
    "\n",
    "\n",
    "    # Call the main scraping function\n",
    "    scrape_urls_from_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Details of the Code\n",
    "\n",
    "**Session with Retry Logic**:  \n",
    "This makes the code more robust by handling intermittent failures when accessing the URLs. It retries the request up to 5 times with increasing delays (exponential backoff).\n",
    "\n",
    "**Browser-Like Headers**:  \n",
    "The headers help disguise the script as a browser, preventing some websites from blocking it.\n",
    "\n",
    "**HTML Parsing with BeautifulSoup**:  \n",
    "The code searches for specific `<span>` elements with a defined class to locate words/keywords and extracts them.\n",
    "\n",
    "**CSV Handling**:  \n",
    "The input CSV is read row by row, URLs are processed, and extracted data is written back into a new CSV with additional columns (`Label 1` to `Label 5`).\n",
    "\n",
    "**Error Handling**:  \n",
    "If a request fails, the error is logged, and the script moves to the next URL, ensuring that an error doesn't stop the entire process.\n",
    "\n",
    "**Delay Between Requests**:  \n",
    "Although set to `0` for now, the `time.sleep(0)` can be adjusted to introduce a delay between requests to avoid overloading a server or getting IP blocked.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
