{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Note Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What the Scraper is Doing\n",
    "\n",
    "The script performs the following tasks:\n",
    "\n",
    "- **Input and Output CSVs**: It reads URLs from an input CSV file and writes the extracted data to an output CSV file.\n",
    "- **Fetching Web Page Content**: For each URL, it sends an HTTP GET request, simulating a browser to avoid being blocked.\n",
    "- **Parsing HTML**: It uses BeautifulSoup to parse the HTML content and searches for specific tags (`span` with class `y-css-kw85nd`).\n",
    "- **Extracting Data**: It retrieves text within anchor (`<a>`) tags nested inside the targeted `span` elements, limited to 5 words per URL.\n",
    "- **Handling Errors**: The script includes error handling for failed requests, ensuring that the script doesn't crash if a URL is unreachable.\n",
    "- **Writing to CSV**: The extracted words are written to specific columns (`Label 1` to `Label 5`) in the output CSV.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. How to Inspect a Website and Find the Information to Extract\n",
    "\n",
    "When you want to scrape data from a website, you first need to understand how the information is structured in the HTML. Here's a step-by-step guide on how to do this:\n",
    "\n",
    "##### 1. Visual Inspection:\n",
    "\n",
    "- **Start by Visiting the Webpage**: Open the webpage in a browser (e.g., Chrome, Firefox).\n",
    "- **Identify the Data You Need**: Look at the page and decide what specific information you want to extract (e.g., product names, prices, reviews, etc.).\n",
    "\n",
    "##### 2. Inspecting the HTML Code:\n",
    "\n",
    "- **Right-Click on the Desired Data**: Find the piece of data you want on the page (e.g., the name of a restaurant). Right-click on it and select \"Inspect\" or \"Inspect Element\" from the context menu.\n",
    "\n",
    "Note: Make sure that the HTML code that you are viewing is for the web browser and not the mobile version. You can change the version by clicking on the dimension (right above the screen once the HTML code is opened) and selecting the right version. \n",
    "\n",
    "\n",
    "- **Examine the HTML Panel**: This opens the developer tools in your browser, highlighting the HTML element that corresponds to the data you selected.\n",
    "  - **Understanding Tags and Attributes**: HTML is made up of various tags like `<div>`, `<span>`, `<a>`, etc. Each tag can have attributes like `class`, `id`, `href`, etc., that help identify and style it.\n",
    "\n",
    "##### 3. Finding Patterns:\n",
    "\n",
    "- **Look for Classes or IDs**: Check if the element has a unique class or id. These attributes are usually consistent across similar elements on the page (e.g., all product names might be in `<span>` tags with a class `product-title`).\n",
    "- **Navigate Up or Down the HTML Structure**: Sometimes, the data is nested inside another tag. You might need to go one level up or down in the HTML hierarchy to find a container that groups several elements you’re interested in.\n",
    "- **Identify Repeating Patterns**: If you’re scraping multiple items (like a list of products), find the common pattern in the HTML structure that repeats for each item.\n",
    "\n",
    "##### 4. Testing Your Selection:\n",
    "\n",
    "- **Use the Console for Testing**: In the browser's developer tools, you can use the Console to write small JavaScript snippets to see if your selection correctly identifies all desired elements.\n",
    "- **Copy the Selector**: Once you’ve identified the correct tag and class or id, you can write the selector in your code to target these elements.\n",
    "\n",
    "This process helps you translate what you see on the webpage into code that can reliably extract the information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://www.yelp.ca/biz/sushi-sama-montr%C3%A9al-11: 403 Client Error: Forbidden for url: https://www.yelp.ca/biz/sushi-sama-montr%C3%A9al-11\n",
      "Test Results:\n",
      "Business Name: \n",
      "Review Rating: \n",
      "Number of Reviews: \n",
      "Food Type: \n",
      "Operational Hours: \n"
     ]
    }
   ],
   "source": [
    "# Incomplete Scraper Script for Students\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "# Function to extract specific words from a given URL\n",
    "def extract_words_from_url(url):\n",
    "    try:\n",
    "        # Create a session object to persist settings across requests\n",
    "        session = requests.Session()\n",
    "\n",
    "        #Retry logic to handle server errors\n",
    "        retries = Retry(\n",
    "            total = 5, \n",
    "            backoff_factor = 1,\n",
    "            status_forcelist = [500, 502, 503, 504] #allow for various error codes\n",
    "        )\n",
    "\n",
    "        #Retry adapter classes\n",
    "        session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "        #Mimic Browser Headers  \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "        }\n",
    "\n",
    "        #Request to the URL using session.get\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Initialize dictionary for extracted data\n",
    "        data = {\n",
    "            'Business Name': '',\n",
    "            'Review Rating': '',\n",
    "            'Number of Reviews': '',\n",
    "            'Food Type': '',\n",
    "            'Operational Hours': ''\n",
    "        }\n",
    "\n",
    "        # Extract business name\n",
    "        name_tag = soup.find('h1', class_='y-css-olzveb')\n",
    "        if name_tag:\n",
    "            data['Business Name'] = name_tag.text.strip()\n",
    "\n",
    "        # Extract review score and count\n",
    "        review_div = soup.find('div', {'data-testid': 'BizHeaderReviewCount'})\n",
    "        if review_div:\n",
    "            rating = review_div.find('span', class_='y-css-1jz061g')\n",
    "            reviews = review_div.find('span', class_='y-css-1cafv3i')\n",
    "            if rating:\n",
    "                data['Review Rating'] = rating.text.strip()\n",
    "            if reviews:\n",
    "                data['Number of Reviews'] = reviews.text.strip('()')\n",
    "\n",
    "        # Extract food/restaurant type\n",
    "        food_type = soup.find('a', class_='y-css-1x1e1r2')\n",
    "        if food_type:\n",
    "            data['Food Type'] = food_type.text.strip()\n",
    "\n",
    "        # Extract operational hours\n",
    "        hours = soup.find('span', {'data-font-weight': 'semibold', 'class': 'y-css-1jz061g'})\n",
    "        if hours:\n",
    "            data['Operational Hours'] = hours.text.strip()\n",
    "\n",
    "        return list(data.values())\n",
    "\n",
    "    except RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return [\"\"] * 5\n",
    "\n",
    "    finally:\n",
    "        time.sleep(1) #added a delay based on code seen to avoid overwhelming the server\n",
    "\n",
    "def scrape_urls_from_csv(input_csv, output_csv):\n",
    "    # # Testing the web scraper with a single URL (to avoid getting IP blocked again LOL)\n",
    "    # test_url = \"https://www.yelp.ca/biz/sushi-sama-montr%C3%A9al-11\"\n",
    "    # extracted_data = extract_words_from_url(test_url)\n",
    "    # print(\"Test Results:\")\n",
    "    # print(\"Business Name:\", extracted_data[0])\n",
    "    # print(\"Review Rating:\", extracted_data[1])\n",
    "    # print(\"Number of Reviews:\", extracted_data[2])\n",
    "    # print(\"Food Type:\", extracted_data[3])\n",
    "    # print(\"Operational Hours:\", extracted_data[4])\n",
    "\n",
    "    with open(input_csv, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "            open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "\n",
    "    # CSV Reader initialization\n",
    "        reader = csv.DictReader(infile)\n",
    "\n",
    "        # Fieldnames for the output CSV\n",
    "        fieldnames = reader.fieldnames + [\n",
    "            'Business Name',\n",
    "            'Review Rating',\n",
    "            'Number of Reviews',\n",
    "            'Food Type',\n",
    "            'Operational Hours'\n",
    "        ]\n",
    "\n",
    "        # CSV Writer initialization\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames) \n",
    "        writer.writeheader()\n",
    "\n",
    "        # Process each row\n",
    "        for row in reader:\n",
    "            url = row.get('restaurant_url')\n",
    "            \n",
    "            if not url:\n",
    "                print(f\"Skipping row with missing URL: {row}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing URL: {url}\")\n",
    "            \n",
    "            # Extract business data\n",
    "            extracted_data = extract_words_from_url(url)\n",
    "            \n",
    "            # Add business data to row\n",
    "            row['Business Name'] = extracted_data[0]\n",
    "            row['Review Rating'] = extracted_data[1]\n",
    "            row['Number of Reviews'] = extracted_data[2]\n",
    "            row['Food Type'] = extracted_data[3]\n",
    "            row['Operational Hours'] = extracted_data[4]\n",
    "            \n",
    "            # Write row once\n",
    "            writer.writerow(row)\n",
    "            time.sleep(1)  # delay to avoid overloading server \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"/Users/hari/Documents/INSY 669/Module 1/INSY669_module_1/URLs to scrape.csv\"\n",
    "    output_csv = \"/Users/hari/Documents/INSY 669/Module 1/INSY669_module_1/scraped_results.csv\"\n",
    "    # Call the main scraping function\n",
    "    scrape_urls_from_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Details of the Code\n",
    "\n",
    "**Session with Retry Logic**:  \n",
    "This makes the code more robust by handling intermittent failures when accessing the URLs. It retries the request up to 5 times with increasing delays (exponential backoff).\n",
    "\n",
    "**Browser-Like Headers**:  \n",
    "The headers help disguise the script as a browser, preventing some websites from blocking it.\n",
    "\n",
    "**HTML Parsing with BeautifulSoup**:  \n",
    "The code searches for specific `<span>` elements with a defined class to locate words/keywords and extracts them.\n",
    "\n",
    "**CSV Handling**:  \n",
    "The input CSV is read row by row, URLs are processed, and extracted data is written back into a new CSV with additional columns (`Label 1` to `Label 5`).\n",
    "\n",
    "**Error Handling**:  \n",
    "If a request fails, the error is logged, and the script moves to the next URL, ensuring that an error doesn't stop the entire process.\n",
    "\n",
    "**Delay Between Requests**:  \n",
    "Although set to `0` for now, the `time.sleep(0)` can be adjusted to introduce a delay between requests to avoid overloading a server or getting IP blocked.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
